{
	"testMillis": 5400000,
	"testTime": true,
	"questions": [
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "Your company policies require encryption of sensitive data at rest. You are considering the possible options for protecting data while storing it at rest on an EBS data volume, attached to an EC2 instance.\nWhich of these options would allow you to encrypt your data at rest? (Choose 3)",
			"type": "checkbox",
			"showNumAnswers": true,
			"options": [
				"Implement third party volume encryption tools",
				"Implement SSL/TLS for all services running on the server",
				"Encrypt data inside your applications before storing it on EBS",
				"Encrypt data using native data encryption drivers at the file system level",
				"Do nothing as EBS volumes are encrypted by default"
			],
			"answers": [
				"Implement third party volume encryption tools",
				"Encrypt data inside your applications before storing it on EBS",
				"Encrypt data using native data encryption drivers at the file system level"
			],
			"answerDescription": ""
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "A customer is deploying an SSL enabled web application to AWS and would like to implement a separation of roles between the EC2 service administrators that are entitled to login to instances as well as making API calls and the security officers who will maintain and have exclusive access to the application’s X.509 certificate that contains the private key.",
			"type": "radio",
			"showNumAnswers": false,
			"options": [
				"Upload the certificate on an S3 bucket owned by the security officers and accessible only by EC2 Role of the web servers",
				"Configure the web servers to retrieve the certificate upon boot from an CloudHSM is managed by the security officers",
				"Configure system permissions on the web servers to restrict access to the certificate only to the authority security officers",
				"Configure IAM policies authorizing access to the certificate store only to the security officers and terminate SSL on an ELB"
			],
			"answers": [
				"Configure IAM policies authorizing access to the certificate store only to the security officers and terminate SSL on an ELB"
			],
			"answerDescription": "You'll terminate the SSL at ELB. and the web request will get unencrypted to the EC2 instance, even if the certs are stored in S3, it has to be configured on the web servers or load balancers somehow, which becomes difficult if the keys are stored in S3. However, keeping the keys in the cert store and using IAM to restrict access gives a clear separation of concern between security officers and developers. Developer’s personnel can still configure SSL on ELB without actually handling the keys."
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "You have recently joined a startup company building sensors to measure street noise and air quality in urban areas. The company has been running a pilot deployment of around 100 sensors for 3 months each sensor uploads 1KB of sensor data every minute to a backend hosted on AWS.\nDuring the pilot, you measured a peak or 10 IOPS on the database, and you stored an average of 3GB of sensor data per month in the database.\nThe current deployment consists of a load-balanced auto scaled Ingestion layer using EC2 instances and a PostgreSQL RDS database with 500GB standard storage.\nThe pilot is considered a success and your CEO has managed to get the attention or some potential investors. The business plan requires a deployment of at least 100K sensors which needs to be supported by the backend. You also need to store sensor data for at least two years to be able to compare year over year Improvements.\n To secure funding, you have to make sure that the platform meets these requirements and leaves room for further scaling.\nWhich setup win meet the requirements?",
			"type": "radio",
			"showNumAnswers": false,
			"options": [
				"Add an SQS queue to the ingestion layer to buffer writes to the RDS instance",
				"Ingest data into a DynamoDB table and move old data to a Redshift cluster",
				"Replace the RDS instance with a 6 node Redshift cluster with 96TB of storage",
				"Keep the current architecture but upgrade RDS storage to 3TB and 10K provisioned IOPS"
			],
			"answers": [
				"Replace the RDS instance with a 6 node Redshift cluster with 96TB of storage"
			],
			"answerDescription": "The POC solution is being scaled up by 1000, which means it will require 72TB of Storage to retain 24 months’ worth of data. This rules out RDS as a possible DB solution which leaves you with Redshift. I believe DynamoDB is a more cost effective and scales better for ingest rather than using EC2 in an auto scaling group.\nAlso, this example solution from AWS is somewhat similar for reference.\nhttp://media.amazonwebservices.com/architecturecenter/AWS_ac_ra_timeseriesprocessing_16.pdf"
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "A web company is looking to implement an intrusion detection and prevention system into their deployed VPC. This platform should have the ability to scale to thousands of instances running inside of the VPC.\nHow should they architect their solution to achieve these goals?",
			"type": "radio",
			"showNumAnswers": false,
			"options": [
				"Configure an instance with monitoring software and the elastic network interface (ENI) set to promiscuous mode packet sniffing to see an traffic across the VPC",
				"Create a second VPC and route all traffic from the primary application VPC through the second VPC where the scalable virtualized IDS/IPS platform resides",
				"Configure servers running in the VPC using the host-based 'route' commands to send all traffic through the platform to a scalable virtualized IDS/IPS",
				"Configure each host with an agent that collects all network traffic and sends that traffic to the IDS/IPS platform for inspection"
			],
			"answers": [
				"Configure each host with an agent that collects all network traffic and sends that traffic to the IDS/IPS platform for inspection"
			],
			"answerDescription": ""
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "A company is storing data on Amazon Simple Storage Service (S3). The company's security policy mandates that data is encrypted at rest.\nWhich of the following methods can achieve this? (Choose 3)",
			"type": "checkbox",
			"showNumAnswers": true,
			"options": [
				"Use Amazon S3 server-side encryption with AWS Key Management Service managed keys",
				"Use Amazon S3 server-side encryption with customer-provided keys",
				"Use Amazon S3 server-side encryption with EC2 key pair",
				"Use Amazon S3 bucket policies to restrict access to the data at rest",
				"Encrypt the data on the client-side before ingesting to Amazon S3 using their own master key",
				"Use SSL to encrypt the data while in transit to Amazon S3"
			],
			"answers": [
				"Use Amazon S3 server-side encryption with AWS Key Management Service managed keys",
				"Use Amazon S3 server-side encryption with customer-provided keys",
				"Encrypt the data on the client-side before ingesting to Amazon S3 using their own master key"
			],
			"answerDescription": "http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html"
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "Your firm has uploaded a large amount of aerial image data to S3. In the past, in your on-premises environment, you used a dedicated group of servers to oaten process this data and used Rabbit MQ - An open source messaging system to get job information to the servers. Once processed the data would go to tape and be shipped offsite. Your manager told you to stay with the current design, and leverage AWS archival storage and messaging services to minimize cost.\nWhich is correct?",
			"type": "radio",
			"showNumAnswers": false,
			"options": [
				"Use SQS for passing job messages use Cloud Watch alarms to terminate EC2 worker instances when they become idle. Once data is processed, change the storage class of the S3 objects to Reduced Redundancy Storage",
				"Setup Auto-Scaled workers triggered by queue depth that use spot instances to process messages in SOS Once data is processed, change the storage class of the S3 objects to Reduced Redundancy Storage",
				"Setup Auto-Scaled workers triggered by queue depth that use spot instances to process messages in SQS Once data is processed, change the storage class of the S3 objects to Glacier",
				"Use SNS to pass job messages use Cloud Watch alarms to terminate spot worker instances when they become idle. Once data is processed, change the storage class of the S3 object to Glacier"
			],
			"answers": [
				"Setup Auto-Scaled workers triggered by queue depth that use spot instances to process messages in SQS Once data is processed, change the storage class of the S3 objects to Glacier"
			],
			"answerDescription": ""
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "You've been hired to enhance the overall security posture for a very large e-commerce site. They have a well architected multi-tier application running in a VPC that uses ELBs in front of both the web and the app tier with static assets served directly from S3. They are using a combination of RDS and DynamoDB for their dynamic data and then archiving nightly into S3 for further processing with EMR. They are concerned because they found questionable log entries and suspect someone is attempting to gain unauthorized access.\nWhich approach provides a cost effective scalable mitigation to this kind of attack?",
			"type": "radio",
			"showNumAnswers": false,
			"options": [
				"Recommend that they lease space at a DirectConnect partner location and establish a 1G DirectConnect connection to their VPC they would then establish Internet connectivity into their space, filter the traffic in hardware Web Application Firewall (WAF). And then pass the traffic through the DirectConnect connection into their application running in their VPC",
				"Add previously identified hostile source IPs as an explicit INBOUND DENY NACL to the web tier subnet",
				"Add a WAF tier by creating a new ELB and an AutoScaling group of EC2 Instances running a hostbased WAF. They would redirect Route 53 to resolve to the new WAF tier ELB. The WAF tier would their pass the traffic to the current web tier The web tier Security Groups would be updated to only allow traffic from the WAF tier Security Group",
				"Remove all but TLS 1.2 from the web tier ELB and enable Advanced Protocol Filtering. This will enable the ELB itself to perform WAF functionality"
			],
			"answers": [
				"Add a WAF tier by creating a new ELB and an AutoScaling group of EC2 Instances running a hostbased WAF. They would redirect Route 53 to resolve to the new WAF tier ELB. The WAF tier would their pass the traffic to the current web tier The web tier Security Groups would be updated to only allow traffic from the WAF tier Security Group"
			],
			"answerDescription": ""
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "Your company is in the process of developing a next generation pet collar that collects biometric information to assist families with promoting healthy lifestyles for their pets. Each collar will push 30kb of biometric data in JSON format every 2 seconds to a collection platform that will process and analyze the data providing health trending information back to the pet owners and veterinarians via a web portal. Management has tasked you to architect the collection platform ensuring the following requirements are met.\n1. Provide the ability for real-time analytics of the inbound biometric data\n2. Ensure processing of the biometric data is highly durable. Elastic and parallel\n3. The results of the analytic processing should be persisted for data mining\nWhich architecture outlined below win meet the initial requirements for the collection platform?",
			"type": "radio",
			"showNumAnswers": false,
			"options": [
				"Utilize S3 to collect the inbound sensor data analyze the data from S3 with a daily scheduled Data Pipeline and save the results to a Redshift Cluster",
				"Utilize Amazon Kinesis to collect the inbound sensor data, analyze the data with Kinesis clients and save the results to a Redshift cluster using EMR",
				"Utilize SQS to collect the inbound sensor data analyze the data from SQS with Amazon Kinesis and save the results to a Microsoft SQL Server RDS instance",
				"Utilize EMR to collect the inbound sensor data, analyze the data from EUR with Amazon Kinesis and save me results to DynamoDB"
			],
			"answers": [
				"Utilize Amazon Kinesis to collect the inbound sensor data, analyze the data with Kinesis clients and save the results to a Redshift cluster using EMR"
			],
			"answerDescription": ""
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "You are designing Internet connectivity for your VPC. The Web servers must be available on the Internet. The application must have a highly available architecture.\nWhich alternatives should you consider? (Choose 2)",
			"type": "checkbox",
			"showNumAnswers": true,
			"options": [
				"Configure a NAT instance in your VPC. Create a default route via the NAT instance and associate it with all subnets. Configure a DNS A record that points to the NAT instance public IP address",
				"Configure a CloudFront distribution and configure the origin to point to the private IP addresses of your Web servers. Configure a Route53 CNAME record to your CloudFront distribution",
				"Place all your web servers behind ELB. Configure a Route53 CNMIE to point to the ELB DNS name",
				"Assign EIPs to all web servers. Configure a Route53 record set with all EIPs, with health checks and DNS failover",
				"Configure ELB with an EIP. Place all your Web servers behind ELB. Configure a Route53 A record that points to the EIP"
			],
			"answers": [
				"Place all your web servers behind ELB. Configure a Route53 CNMIE to point to the ELB DNS name",
				"Assign EIPs to all web servers. Configure a Route53 record set with all EIPs, with health checks and DNS failover"
			],
			"answerDescription": ""
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "Your team has a tomcat-based Java application you need to deploy into development, test and production environments. After some research, you opt to use Elastic Beanstalk due to its tight integration with your developer tools and RDS due to its ease of management. Your QA team lead points out that you need to roll a sanitized set of production data into your environment on a nightly basis. Similarly, other software teams in your org want access to that same restored data via their EC2 instances in your VPC.\nThe optimal setup for persistence and security that meets the above requirements would be the following",
			"type": "radio",
			"showNumAnswers": false,
			"options": [
				"Create your RDS instance as part of your Elastic Beanstalk definition and alter its security group to allow access to it from hosts in your application subnets",
				"Create your RDS instance separately and add its IP address to your application's DB connection strings in your code Alter its security group to allow access to it from hosts within your VPC's IP address block",
				"Create your RDS instance separately and pass its DNS name to your app's DB connection string as an environment variable. Create a security group for client machines and add it as a valid source for DB traffic to the security group of the RDS instance itself",
				"Create your RDS instance separately and pass its DNS name to your's DB connection string as an environment variable Alter its security group to allow access to It from hosts in your application subnets"
			],
			"answers": [
				"Create your RDS instance as part of your Elastic Beanstalk definition and alter its security group to allow access to it from hosts in your application subnets"
			],
			"answerDescription": ""
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "Your company has an on-premises multi-tier PHP web application, which recently experienced downtime due to a large burst in web traffic due to a company announcement Over the coming days, you are expecting similar announcements to drive similar unpredictable bursts, and are looking to find ways to quickly improve your infrastructures ability to handle unexpected increases in traffic. The application currently consists of 2 tiers a web tier which consists of a load balancer and several Linux Apache web servers as well as a database tier which hosts a Linux server hosting a MySQL database.\nWhich scenario below will provide full site functionality, while helping to improve the ability of your application in the short timeframe required?",
			"type": "radio",
			"showNumAnswers": false,
			"options": [
				"Failover environment: Create an S3 bucket and configure it for website hosting. Migrate your DNS to Route53 using zone file import, and leverage Route53 DNS failover to failover to the S3 hosted website",
				"Hybrid environment: Create an AMI, which can be used to launch web servers in EC2. Create an Auto Scaling group, which uses the AMI to scale the web tier based on incoming traffic. Leverage Elastic Load Balancing to balance traffic between on-premises web servers and those hosted in AWS",
				"Offload traffic from on-premises environment: Setup a CIoudFront distribution, and configure CloudFront to cache objects from a custom origin. Choose to customize your object cache behavior, and select a TTL that objects should exist in cache",
				"Migrate to AWS: Use VM Import/Export to quickly convert an on-premises web server to an AMI. Create an Auto Scaling group, which uses the imported AMI to scale the web tier based on incoming traffic. Create an RDS read replica and setup replication between the RDS instance and on-premises MySQL server to migrate the database"
			],
			"answers": [
				"Offload traffic from on-premises environment: Setup a CIoudFront distribution, and configure CloudFront to cache objects from a custom origin. Choose to customize your object cache behavior, and select a TTL that objects should exist in cache"
			],
			"answerDescription": ""
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "You are implementing AWS Direct Connect. You intend to use AWS public service end points such as Amazon S3, across the AWS Direct Connect link. You want other Internet traffic to use your existing link to an Internet Service Provider.\nWhat is the correct way to configure AWS Direct connect for access to services such as Amazon S3?",
			"type": "radio",
			"showNumAnswers": false,
			"options": [
				"Configure a public Interface on your AWS Direct Connect link. Configure a static route via your AWS Direct Connect link that points to Amazon S3 Advertise a default route to AWS using BGP",
				"Create a private interface on your AWS Direct Connect link. Configure a static route via your AWS Direct connect link that points to Amazon S3 Configure specific routes to your network in your VPC",
				"Create a public interface on your AWS Direct Connect link. Redistribute BGP routes into your existing routing infrastructure; advertise specific routes for your network to AWS",
				"Create a private interface on your AWS Direct connect link. Redistribute BGP routes into your existing routing infrastructure and advertise a default route to AWS"
			],
			"answers": [
				"Create a public interface on your AWS Direct Connect link. Redistribute BGP routes into your existing routing infrastructure; advertise specific routes for your network to AWS"
			],
			"answerDescription": ""
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "Your application is using an ELB in front of an Auto Scaling group of web/application servers deployed across two AZs and a Multi-AZ RDS Instance for data persistence. The database CPU is often above 80% usage and 90% of I/O operations on the database are reads. To improve performance you recently added a single-node Memcached ElastiCache Cluster to cache frequent DB query results. In the next weeks the overall workload is expected to grow by 30%.\nDo you need to change anything in the architecture to maintain the high availability or the application with the anticipated additional load? Why?",
			"type": "radio",
			"showNumAnswers": false,
			"options": [
				"Yes, you should deploy two Memcached ElastiCache Clusters in different AZs because the RDS instance will not be able to handle the load if the cache node fails",
				"No, if the cache node fails you can always get the same data from the DB without having any availability impact",
				"No, if the cache node fails the automated ElastiCache node recovery feature will prevent any availability impact",
				"Yes, you should deploy the Memcached ElastiCache Cluster with two nodes in the same AZ as the RDS DB master instance to handle the load if one cache node fails."
			],
			"answers": [
				"Yes, you should deploy two Memcached ElastiCache Clusters in different AZs because the RDS instance will not be able to handle the load if the cache node fails"
			],
			"answerDescription": ""
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "An ERP application is deployed across multiple AZs in a single region. In the event of failure, the Recovery Time Objective (RTO) must be less than 3 hours, and the Recovery Point Objective (RPO) must be 15 minutes. The customer realizes that data corruption occurred roughly 1.5 hours ago.\nWhat DR strategy could be used to achieve this RTO and RPO in the event of this kind of failure?",
			"type": "radio",
			"showNumAnswers": false,
			"options": [
				"Take hourly DB backups to S3, with transaction logs stored in S3 every 5 minutes",
				"Use synchronous database master-slave replication between two availability zones",
				"Take hourly DB backups to EC2 Instance store volumes with transaction logs stored In S3 every 5 minutes",
				"Take 15 minute DB backups stored In Glacier with transaction logs stored in S3 every 5 minutes"
			],
			"answers": [
				"Take hourly DB backups to S3, with transaction logs stored in S3 every 5 minutes"
			],
			"answerDescription": ""
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "You are designing the network infrastructure for an application server in Amazon VPC. Users will access all application instances from the Internet, as well as from an on-premises network. The on-premises network is connected to your VPC over an AWS Direct Connect link.\nHow would you design routing to meet the above requirements?",
			"type": "radio",
			"showNumAnswers": false,
			"options": [
				"Configure a single routing table with a default route via the Internet gateway. Propagate a default route via BGP on the AWS Direct Connect customer router. Associate the routing table with all VPC subnets",
				"Configure a single routing table with a default route via the Internet gateway. Propagate specific routes for the on-premises networks via BGP on the AWS Direct Connect customer router. Associate the routing table with all VPC subnets",
				"Configure a single routing table with two default routes: on to the Internet via an Internet gateway, the other to the on-premises network via the VPN gateway. Use this routing table across all subnets in the VPC",
				"Configure two routing tables: on that has a default router via the Internet gateway, and other that has a default route via the VPN gateway. Associate both routing tables with each VPC subnet"
			],
			"answers": [
				"Configure a single routing table with a default route via the Internet gateway. Propagate specific routes for the on-premises networks via BGP on the AWS Direct Connect customer router. Associate the routing table with all VPC subnets"
			],
			"answerDescription": ""
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "You control access to S3 buckets and objects with:",
			"type": "radio",
			"showNumAnswers": false,
			"options": [
				"Identity and Access Management (IAM) Policies",
				"Access Control Lists (ACLs)",
				"Bucket Policies",
				"All of the above"
			],
			"answers": [
				"All of the above"
			],
			"answerDescription": ""
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "The AWS IT infrastructure that AWS provides, complies with the following IT security standards, including:",
			"type": "radio",
			"showNumAnswers": false,
			"options": [
				"SOC 1/SSAE 16/ISAE 3402 (formerly SAS 70 Type II), SOC 2 and SOC 3",
				"FISMA, DIACAP, and FedRAMP",
				"PCI DSS Level 1, ISO 27001, ITAR and FIPS 140-2",
				"HIPAA, Cloud Security Alliance (CSA) and Motion Picture Association of America (MPAA)",
				"All of the above"
			],
			"answers": [
				"All of the above"
			],
			"answerDescription": ""
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "Auto Scaling requests are signed with a _________ signature calculated from the request and the user’s private key",
			"type": "radio",
			"showNumAnswers": false,
			"options": [
				"SSL",
				"AES-256",
				"HMAC-SHA1",
				"X.509"
			],
			"answers": [
				"HMAC-SHA1"
			],
			"answerDescription": ""
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "What does elasticity mean to AWS?",
			"type": "radio",
			"showNumAnswers": false,
			"options": [
				"The ability to scale computing resources up easily, with minimal friction and down with latency",
				"The ability to scale computing resources up and down easily, with minimal friction",
				"The ability to provision cloud computing resources in expectation of future demand",
				"The ability to recover from business continuity events with minimal friction"
			],
			"answers": [
				"The ability to scale computing resources up and down easily, with minimal friction"
			],
			"answerDescription": ""
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "The following are AWS Storage services? Choose 2 Answers",
			"type": "checkbox",
			"showNumAnswers": true,
			"options": [
				"AWS Relational Database Service (AWS RDS)",
				"AWS ElastiCache",
				"AWS Glacier",
				"AWS Import/Export"
			],
			"answers": [
				"AWS Glacier",
				"AWS Import/Export"
			],
			"answerDescription": ""
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "How is AWS readily distinguished from other vendors in the traditional IT computing landscape?",
			"type": "radio",
			"showNumAnswers": false,
			"options": [
				"Experienced. Scalable and elastic. Secure. Cost-effective. Reliable",
				"Secure. Flexible. Cost-effective. Scalable and elastic. Global",
				"Secure. Flexible. Cost-effective. Scalable and elastic. Experienced",
				"Flexible. Cost-effective. Dynamic. Secure. Experienced."
			],
			"answers": [
				"Secure. Flexible. Cost-effective. Scalable and elastic. Experienced"
			],
			"answerDescription": ""
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "You have launched an EC2 instance with four (4) 500 GB EBS Provisioned IOPS volumes attached. The EC2 instance is EBS-Optimized and supports 500 Mbps throughput between EC2 and EBS. The four EBS volumes are configured as a single RAID 0 device, and each Provisioned IOPS volume is provisioned with 4,000 IOPS (4,000 16KB reads or writes), for a total of 16,000 random IOPS on the instance. The EC2 instance initially delivers the expected 16,000 IOPS random read and write performance. Sometime later, in order to increase the total random I/O performance of the instance, you add an additional two 500 GB EBS Provisioned IOPS volumes to the RAID. Each volume is provisioned to 4,000 IOPs like the original four, for a total of 24,000 IOPS on the EC2 instance. Monitoring shows that the EC2 instance CPU utilization increased from 50% to 70%, but the total random IOPS measured at the instance level does not increase at all.\nWhat is the problem and a valid solution?",
			"type": "radio",
			"showNumAnswers": false,
			"options": [
				"The EBS-Optimized throughput limits the total IOPS that can be utilized; use an EBSOptimized instance that provides larger throughput",
				"Small block sizes cause performance degradation, limiting the I/O throughput; configure the instance device driver and filesystem to use 64KB blocks to increase throughput",
				"The standard EBS Instance root volume limits the total IOPS rate; change the instance root volume to also be a 500GB 4,000 Provisioned IOPS volume",
				"Larger storage volumes support higher Provisioned IOPS rates; increase the provisioned volume storage of each of the 6 EBS volumes to 1TB",
				"RAID 0 only scales linearly to about 4 devices; use RAID 0 with 4 EBS Provisioned IOPS volumes, but increase each Provisioned IOPS EBS volume to 6,000 IOPS"
			],
			"answers": [
				"The standard EBS Instance root volume limits the total IOPS rate; change the instance root volume to also be a 500GB 4,000 Provisioned IOPS volume"
			],
			"answerDescription": ""
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "Your company is storing millions of sensitive transactions across thousands of 100-GB files that must be encrypted in transit and at rest. Analysts concurrently depend on subsets of files, which can consume up to 5 TB of space, to generate simulations that can be used to steer business decisions.\nYou are required to design an AWS solution that can cost effectively accommodate the long-term storage and in-flight subsets of data.\nWhich approach can satisfy these objectives?",
			"type": "radio",
			"showNumAnswers": false,
			"options": [
				"Use Amazon Simple Storage Service (S3) with server-side encryption, and run simulations on subsets in ephemeral drives on Amazon EC2",
				"Use Amazon S3 with server-side encryption, and run simulations on subsets in-memory on Amazon EC2",
				"Use HDFS on Amazon EMR, and run simulations on subsets in ephemeral drives on Amazon EC2",
				"Use HDFS on Amazon Elastic MapReduce (EMR), and run simulations on subsets in-memory on Amazon Elastic Compute Cloud (EC2)",
				"Store the full data set in encrypted Amazon Elastic Block Store (EBS) volumes, and regularly capture snapshots that can be cloned to EC2 workstations"
			],
			"answers": [
				"Use HDFS on Amazon Elastic MapReduce (EMR), and run simulations on subsets in-memory on Amazon Elastic Compute Cloud (EC2)"
			],
			"answerDescription": ""
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "Your customer is willing to consolidate their log streams (access logs, application logs, security logs, etc.) in one single system. Once consolidated, the customer wants to analyze these logs in real time based on heuristics. From time to time, the customer needs to validate heuristics, which requires going back to data samples extracted from the last 12 hours.\nWhat is the best approach to meet your customer’s requirements?",
			"type": "radio",
			"showNumAnswers": false,
			"options": [
				"Send all the log events to Amazon SQS, setup an Auto Scaling group of EC2 servers to consume the logs and apply the heuristics",
				"Send all the log events to Amazon Kinesis, develop a client process to apply heuristics on the logs",
				"Configure Amazon CloudTrail to receive custom logs, use EMR to apply heuristics the logs",
				"Setup an Auto Scaling group of EC2 syslogd servers, store the logs on S3, use EMR to apply heuristics on the logs"
			],
			"answers": [
				"Send all the log events to Amazon Kinesis, develop a client process to apply heuristics on the logs"
			],
			"answerDescription": "The throughput of an Amazon Kinesis stream is designed to scale without limits via increasing the number of shards within a stream. However, there are certain limits you should keep in mind while using Amazon Kinesis Streams:\nBy default, Records of a stream are accessible for up to 24 hours from the time they are added to the stream. You can raise this limit to up to 7 days by enabling extended data retention.\nThe maximum size of a data blob (the data payload before Base64-encoding) within one record is 1 megabyte (MB).\nEach shard can support up to 1000 PUT records per second.\nFor more information about other API level limits, see Amazon Kinesis Streams Limits"
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "A newspaper organization has an on-premises application which allows the public to search its back catalogue and retrieve individual newspaper pages via a website written in Java. They have scanned the old newspapers into JPEGs (approx 17TB) and used Optical Character Recognition (OCR) to populate a commercial search product. The hosting platform and software are now end of life and the organization wants to migrate Its archive to AWS and produce a cost efficient architecture and still be designed for availability and durability.\nWhich is the most appropriate?",
			"type": "radio",
			"showNumAnswers": false,
			"options": [
				"Use S3 with reduced redundancy lo store and serve the scanned files, install the commercial search application on EC2 Instances and configure with auto-scaling and an Elastic Load Balancer",
				"Model the environment using CloudFormation use an EC2 instance running Apache webserver and an open source search application, stripe multiple standard EBS volumes together to store the JPEGs and search index",
				"Use S3 with standard redundancy to store and serve the scanned files, use CloudSearch for query processing, and use Elastic Beanstalk to host the website across multiple availability zones",
				"Use a single-AZ RDS MySQL instance lo store the search index 33d the JPEG images use an EC2 instance to serve the website and translate user queries into SQL",
				"Use a CloudFront download distribution to serve the JPEGs to the end users and Install the current commercial search product, along with a Java Container Tor the website on EC2 instances and use Route53 with DNS round-robin"
			],
			"answers": [
				"Use S3 with standard redundancy to store and serve the scanned files, use CloudSearch for query processing, and use Elastic Beanstalk to host the website across multiple availability zones"
			],
			"answerDescription": "There is no such thing as 'Most appropriate' without knowing all your goals. I find your scenarios very fuzzy, since you can obviously mix-n-match between them. I think you should decide by layers instead:\nLoad Balancer Layer: ELB or just DNS, or roll-your-own. (Using DNS+EIPs is slightly cheaper, but less reliable than ELB.)\nStorage Layer for 17TB of Images: This is the perfect use case for S3. Off-load all the web requests directly to the relevant JPEGs in S3. Your EC2 boxes just generate links to them. \nIf your app already serves it's own images (not links to images), you might start with EFS. But more than likely, you can just setup a web server to re-write or re-direct all JPEG links to S3 pretty easily.\nIf you use S3, don't serve directly from the bucket - Serve via a CNAME in domain you control. That way, you can switch in CloudFront easily. \nEBS will be way more expensive, and you'll need 2x the drives if you need 2 boxes. Yuck. Consider a smaller storage format. For example, JPEG200 or WebP or other tools might make for smaller images. There is also the DejaVu format from a while back.\nCache Layer: Adding CloudFront in front of S3 will help people on the other side of the world -- well, possibly. Typical archives follow a power law. The long tail of requests means that most JPEGs won't be requested enough to be in the cache. So you are only speeding up the most popular objects. You can always wait, and switch in CF later after you know your costs better. (In some cases, it can actually lower costs.) You can also put CloudFront in front of your app, since your archive search results should be fairly static. This will also allow you to run with a smaller instance type, since CF will handle much of the load if you do it right.\nDatabase Layer: A few options:Use whatever your current server does for now, and replace with something else down the road. Don't under-estimate this approach, sometimes it's better to start now and optimize later. Use RDS to run MySQL/Postgres\nI'm not as familiar with ElasticSearch / Cloudsearch, but obviously Cloudsearch will be less maintenance +setup. \nApp Layer: When creating the app layer from scratch, consider CloudFormation and/or OpsWorks. It's extra stuff to learn, but helps down the road.\nJava+Tomcat is right up the alley of ElasticBeanstalk. (Basically EC2 + Autoscale + ELB).\nPreventing Abuse: When you put something in a public S3 bucket, people will hot-link it from their web pages. If you want to prevent that, your app on the EC2 box can generate signed links to S3 that expire in a few hours. Now everyone will be forced to go thru the app, and the app can apply rate limiting, etc. \nSaving money: If you don't mind having downtime: run everything in one AZ (both DBs and EC2s). You can always add servers and AZs down the road, as long as it's architected to be stateless. In fact, you should use multiple regions if you want it to be really robust. use Reduced Redundancy in S3 to save a few hundred bucks per month (Someone will have to 'go fix it' every time it breaks, including having an off-line copy to repair S3.) Buy Reserved Instances on your EC2 boxes to make them cheaper. (Start with the RI market and buy a partially used one to get started.) It's just a coupon saying 'if you run this type of box in this AZ, you will save on the per-hour costs.' You can get 1/2 to 1/3 off easily.\nRewrite the application to use less memory and CPU - that way you can run on fewer/smaller boxes. (May or may not be worth the investment.) If your app will be used very infrequently, you will save a lot of money by using Lambda. I'd be worried that it would be quite slow if you tried to run a Java application on it though. We're missing some information like load, latency expectations from search, indexing speed, size of the search index, etc. But with what you've given us, I would go with S3 as the storage for the files (S3 rocks. It is really, really awesome). If you're stuck with the commercial search application, then on EC2 instances with autoscaling and an ELB. If you are allowed an alternative search engine, Elasticsearch is probably your best bet. I'd run it on EC2 instead of the AWS Elasticsearch service, as IMHO it's not ready yet. Don't autoscale Elasticsearch automatically though, it'll cause all sorts of issues. I have zero experience with CloudSearch so ic an't comment on that. Regardless of which option, I'd use CloudFormation for all of it"
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "Your company has recently extended its datacenter into a VPC on AVVS to add burst computing capacity as needed Members of your Network Operations Center need to be able to go to the AWS Management Console and administer Amazon EC2 instances as necessary You don't want to create new IAM users for each NOC member and make those users sign in again to the AWS Management Console.\nWhich option below will meet the needs for your NOC members?",
			"type": "radio",
			"showNumAnswers": false,
			"options": [
				"Use OAuth 2 0 to retrieve temporary AWS security credentials to enable your NOC members to sign in to the AWS Management Console",
				"Use web Identity Federation to retrieve AWS temporary security credentials to enable your NOC members to sign in to the AWS Management Console",
				"Use your on-premises SAML 2.0-compliant identity provider (IDP) to grant the NOC members federated access to the AWS Management Console via the AWS single sign-on (SSO) endpoint",
				"Use your on-premises SAML 2.0-compliam identity provider (IDP) to retrieve temporary security credentials to enable NOC members to sign in to the AWS Management Console"
			],
			"answers": [
				"Use your on-premises SAML 2.0-compliant identity provider (IDP) to grant the NOC members federated access to the AWS Management Console via the AWS single sign-on (SSO) endpoint"
			],
			"answerDescription": "http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_enable-console-saml.html"
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "You are looking to migrate your Development (Dev) and Test environments to AWS. You have decided to use separate AWS accounts to host each environment. You plan to link each accounts bill to a Master AWS account using Consolidated Billing. To make sure you keep within budget you would like to implement a way for administrators in the Master account to have access to stop, delete and/or terminate resources in both the Dev and Test accounts.\nIdentify which option will allow you to achieve this goal?",
			"type": "radio",
			"showNumAnswers": false,
			"options": [
				"Create IAM users in the Master account with full Admin permissions. Create cross-account roles in the Dev and Test accounts that grant the Master account access to the resources in the account by inheriting permissions from the Master account",
				"Create IAM users and a cross-account role in the Master account that grants full Admin permissions to the Dev and Test accounts",
				"Create IAM users in the Master account. Create cross-account roles in the Dev and Test accounts that have full Admin permissions and grant the Master account access",
				"Link the accounts using Consolidated Billing. This will give IAM users in the Master account access to resources in the Dev and Test accounts"
			],
			"answers": [
				"Create IAM users in the Master account. Create cross-account roles in the Dev and Test accounts that have full Admin permissions and grant the Master account access"
			],
			"answerDescription": ""
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "You're running an application on-premises due to its dependency on non-x86 hardware and want to use AWS for data backup. Your backup application is only able to write to POSIX-compatible block-based storage. You have 140TB of data and would like to mount it as a single folder on your file server. Users must be able to access portions of this data while the backups are taking place.\nWhat backup solution would be most appropriate for this use case?",
			"type": "radio",
			"showNumAnswers": false,
			"options": [
				"Use Storage Gateway and configure it to use Gateway Cached volumes",
				"Configure your backup software to use S3 as the target for your data backups",
				"Configure your backup software to use Glacier as the target for your data backups",
				"Use Storage Gateway and configure it to use Gateway Stored volumes"
			],
			"answers": [
				"Use Storage Gateway and configure it to use Gateway Stored volumes"
			],
			"answerDescription": "Volume gateway provides an iSCSI target, which enables you to create volumes and mount them as iSCSI devices from your on-premises application servers. The volume gateway runs in either a cached or stored mode.\nIn the cached mode, your primary data is written to S3, while you retain some portion of it locally in a cache for frequently accessed data.\nIn the stored mode, your primary data is stored locally and your entire dataset is available for low-latency access while asynchronously backed up to AWS.\nIn either mode, you can take point-in-time snapshots of your volumes and store them in Amazon S3, enabling you to make space-efficient versioned copies of your volumes for data protection and various data reuse needs"
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "To serve Web traffic for a popular product your chief financial officer and IT director have purchased 10 m1.large heavy utilization Reserved Instances (RIs), evenly spread across two availability zones; Route 53 is used to deliver the traffic to an Elastic Load Balancer (ELB). After several months, the product grows even more popular and you need additional capacity. As a result, your company purchases two C3.2xlarge medium utilization Ris. You register the two c3.2xlarge instances with your ELB and quickly find that the m1.large instances are at 100% of capacity and the c3.2xlarge instances have significant capacity that's unused.\nWhich option is the most cost effective and uses EC2 capacity most effectively?",
			"type": "radio",
			"showNumAnswers": false,
			"options": [
				"Configure Autoscaling group and Launch Configuration with ELB to add up to 10 more on-demand m1.large instances when triggered by Cloudwatch. Shut off c3.2xlarge instances",
				"Configure ELB with two c3.2xlarge instances and use on-demand Autoscaling group for up to two additional c3.2xlarge instances. Shut off m1.large instances",
				"Route traffic to EC2 m1.large and c3.2xlarge instances directly using Route 53 latency based routing and health checks. Shut off ELB",
				"Use a separate ELB for each instance type and distribute load to ELBs with Route 53 weighted round robin"
			],
			"answers": [
				"Use a separate ELB for each instance type and distribute load to ELBs with Route 53 weighted round robin"
			],
			"answerDescription": "http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html"
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "You have deployed a web application targeting a global audience across multiple AWS Regions under the domain name.example.com. You decide to use Route53 Latency-Based Routing to serve web requests to users from the region closest to the user. To provide business continuity in the event of server downtime you configure weighted record sets associated with two web servers in separate Availability Zones per region. Dunning a DR test you notice that when you disable all web servers in one of the regions Route53 does not automatically direct all users to the other region.\nWhat could be happening? (Choose 2 answers)",
			"type": "checkbox",
			"showNumAnswers": true,
			"options": [
				"Latency resource record sets cannot be used in combination with weighted resource record sets",
				"You did not setup an HTTP health check to one or more of the weighted resource record sets associated with me disabled web servers",
				"The value of the weight associated with the latency alias resource record set in the region with the disabled servers is higher than the weight for the other region",
				"One of the two working web servers in the other region did not pass its HTTP health check",
				"You did not set 'Evaluate Target Health' to 'Yes' on the latency alias resource record set associated with example com in the region where you disabled the servers"
			],
			"answers": [
				"You did not setup an HTTP health check to one or more of the weighted resource record sets associated with me disabled web servers",
				"You did not set 'Evaluate Target Health' to 'Yes' on the latency alias resource record set associated with example com in the region where you disabled the servers"
			],
			"answerDescription": ""
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "Your startup wants to implement an order fulfillment process for selling a personalized gadget that needs an average of 3-4 days to produce with some orders taking up to 6 months you expect 10 orders per day on your first day. 1000 orders per day after 6 months and 10,000 orders after 12 months.\nOrders coming in are checked for consistency men dispatched to your manufacturing plant for production quality control packaging shipment and payment processing If the product does not meet the quality standards at any stage of the process employees may force the process to repeat a step Customers are notified via email about order status and any critical issues with their orders such as payment failure.Your base architecture includes AWS Elastic Beanstalk for your website with an RDS MySQL instance for customer data and orders.\nHow can you implement the order fulfillment process while making sure that the emails are delivered reliably?",
			"type": "radio",
			"showNumAnswers": false,
			"options": [
				"Add a business process management application to your Elastic Beanstalk app servers and re-use the ROS database for tracking order status use one of the Elastic Beanstalk instances to send emails to customers",
				"Use SWF with an Auto Scaling group of activity workers and a decider instance in another Auto Scaling group with min/max=1 Use the decider instance to send emails to customers",
				"Use SWF with an Auto Scaling group of activity workers and a decider instance in another Auto Scaling group with min/max=1 use SES to send emails to customers",
				"Use an SQS queue to manage all process tasks Use an Auto Scaling group of EC2 Instances that poll the tasks and execute them. Use SES to send emails to customers"
			],
			"answers": [
				"Use SWF with an Auto Scaling group of activity workers and a decider instance in another Auto Scaling group with min/max=1 use SES to send emails to customers"
			],
			"answerDescription": ""
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "A read only news reporting site with a combined web and application tier and a database tier that receives large and unpredictable traffic demands must be able to respond to these traffic fluctuations automatically.\nWhat AWS services should be used meet these requirements?",
			"type": "radio",
			"showNumAnswers": false,
			"options": [
				"Stateless instances for the web and application tier synchronized using ElastiCache Memcached in an autoscaimg group monitored with CloudWatch and RDS with read replicas",
				"Stateful instances for the web and application tier in an autoscaling group monitored with CloudWatch and RDS with read replicas",
				"Stateful instances for the web and application tier in an autoscaling group monitored with CloudWatch and multi-AZ RDS",
				"Stateless instances for the web and application tier synchronized using ElastiCache Memcached in an autoscaling group monitored with CloudWatch and multi-AZ RDS"
			],
			"answers": [
				"Stateless instances for the web and application tier synchronized using ElastiCache Memcached in an autoscaimg group monitored with CloudWatch and RDS with read replicas"
			],
			"answerDescription": ""
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "You are designing a photo-sharing mobile app. The application will store all pictures in a single Amazon S3 bucket.\nUsers will upload pictures from their mobile device directly to Amazon S3 and will be able to view and download their own pictures directly from Amazon S3.\nYou want to configure security to handle potentially millions of users in the most secure manner possible.\nWhat should your server-side application do when a new user registers on the photo-sharing mobile application?",
			"type": "radio",
			"showNumAnswers": false,
			"options": [
				"Create an IAM user. Update the bucket policy with appropriate permissions for the IAM user. Generate an access key and secret key for the IAM user, store them in the mobile app and use these credentials to access Amazon S3",
				"Create an IAM user. Assign appropriate permissions to the IAM user. Generate an access key and secret key for the IAM user, store them in the mobile app and use these credentials to access Amazon S3",
				"Create a set of long-term credentials using AWS Security Token Service with appropriate permissions. Store these credentials in the mobile app and use them to access Amazon S3",
				"Record the user's information in Amazon RDS and create a role in IAM with appropriate permissions. When the user uses their mobile app, create temporary credentials using the AWS Security Token Service 'AssumeRole' function. Store these credentials in the mobile app’s memory and use them to access Amazon S3. Generate new credentials the next time the user runs the mobile app",
				"Record the user's information in Amazon DynamoDB. When the user uses their mobile app, create temporary credentials using AWS Security Token Service with appropriate permissions. Store these credentials in the mobile app's memory and use them to access Amazon S3. Generate new credentials the next time the user runs the mobile app"
			],
			"answers": [
				"Record the user's information in Amazon RDS and create a role in IAM with appropriate permissions. When the user uses their mobile app, create temporary credentials using the AWS Security Token Service 'AssumeRole' function. Store these credentials in the mobile app’s memory and use them to access Amazon S3. Generate new credentials the next time the user runs the mobile app"
			],
			"answerDescription": ""
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "You are tasked with moving a legacy application from a virtual machine running inside your datacenter to an Amazon VPC. Unfortunately, this app requires access to a number of on-premises services and no one who configured the app still works for your company. Even worse there's no documentation for it.\nWhat will allow the application running inside the VPC to reach back and access its internal dependencies without being reconfigured? (Choose 3 answers)",
			"type": "checkbox",
			"showNumAnswers": true,
			"options": [
				"An AWS Direct Connect link between the VPC and the network housing the internal services",
				"An Internet Gateway to allow a VPN connection",
				"An Elastic IP address on the VPC instance",
				"An IP address space that does not conflict with the one on-premises",
				"Entries in Amazon Route 53 that allow the Instance to resolve its dependencies' IP addresses",
				"A VM Import of the current virtual machine"
			],
			"answers": [
				"An AWS Direct Connect link between the VPC and the network housing the internal services",
				"An IP address space that does not conflict with the one on-premises",
				"A VM Import of the current virtual machine"
			],
			"answerDescription": ""
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "You have a periodic image analysis application that gets some files in input, analyzes them and tor each file writes some data in output to a ten file the number of files in input per day is high and concentrated in a few hours of the day.\nCurrently you have a server on EC2 with a large EBS volume that hosts the input data and the results. It takes almost 20 hours per day to complete the process.\nWhat services could be used to reduce the elaboration time and improve the availability of the solution?",
			"type": "radio",
			"showNumAnswers": false,
			"options": [
				"S3 to store I/O files. SQS to distribute elaboration commands to a group of hosts working in parallel. Auto scaling to dynamically size the group of hosts depending on the length of the SQS queue",
				"EBS with Provisioned IOPS (PIOPS) to store I/O files. SNS to distribute elaboration commands to a group of hosts working in parallel Auto Scaling to dynamically size the group of hosts depending on the number of SNS notifications",
				"S3 to store I/O files, SNS to distribute evaporation commands to a group of hosts working in parallel. Auto scaling to dynamically size the group of hosts depending on the number of SNS notifications",
				"EBS with Provisioned IOPS (PIOPS) to store I/O files SQS to distribute elaboration commands to a group of hosts working in parallel Auto Scaling to dynamically size the group ot hosts depending on the length of the SQS queue"
			],
			"answers": [
				"EBS with Provisioned IOPS (PIOPS) to store I/O files SQS to distribute elaboration commands to a group of hosts working in parallel Auto Scaling to dynamically size the group ot hosts depending on the length of the SQS queue"
			],
			"answerDescription": "Amazon EBS allows you to create storage volumes and attach them to Amazon EC2 instances. Once attached, you can create a file system on top of these volumes, run a database, or use them in any other way you would use a block device. Amazon EBS volumes are placed in a specific Availability Zone, where they are automatically replicated to protect you from the failure of a single component.\nAmazon EBS provides three volume types: General Purpose (SSD), Provisioned IOPS (SSD), and Magnetic. The three volume types differ in performance characteristics and cost, so you can choose the right storage performance and price for the needs of your applications. All EBS volume types offer the same durable snapshot capabilities and are designed for 99.999% availability"
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "You have been asked to design the storage layer for an application. The application requires disk performance of at least 100,000 IOPS. In addition, the storage layer must be able to survive the loss of an individual disk, EC2 instance, or Availability Zone without any data loss. The volume you provide must have a capacity of at least 3 TB.\nWhich of the following designs will meet these objectives?",
			"type": "radio",
			"showNumAnswers": false,
			"options": [
				"Instantiate a c3.8xlarge instance in us-east-1. Provision 4x1TB EBS volumes, attach them to the instance, and configure them as a single RAID 5 volume. Ensure that EBS snapshots are performed every 15 minutes",
				"Instantiate a c3.8xlarge instance in us-east-1. Provision 3xlTB EBS volumes, attach them to the Instance, and configure them as a single RAID 0 volume. Ensure that EBS snapshots are performed every 15 minutes",
				"Instantiate an i2.8xlarge instance in us-east-1a. Create a RAID 0 volume using the four 800GB SSD ephemeral disks provided with the instance. Provision 3x1TB EBS volumes, attach them to the instance, and configure them as a second RAID 0 volume. Configure synchronous, block-level replication from the ephemeral-backed volume to the EBS-backed volume",
				"Instantiate a c3.8xlarge instance in us-east-1. Provision an AWS Storage Gateway and configure it for 3 TB of storage and 100,000 IOPS. Attach the volume to the instance",
				"Instantiate an i2.8xlarge instance in us-east-1a. Create a RAID 0 volume using the four 800GB SSD ephemeral disks provided with the instance. Configure synchronous, block-level replication to an identically configured instance in us-east-1b"
			],
			"answers": [
				"Instantiate an i2.8xlarge instance in us-east-1a. Create a RAID 0 volume using the four 800GB SSD ephemeral disks provided with the instance. Configure synchronous, block-level replication to an identically configured instance in us-east-1b"
			],
			"answerDescription": "https://acloud.guru/course/aws-certified-solutions-architect-associate/discuss/-KJdi4tFMp2x_O88J6U4/an-architecture-design-question"
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "You are the new IT architect in a company that operates a mobile sleep tracking application. When activated at night, the mobile app is sending collected data points of 1 kilobyte every 5 minutes to your backend.\nThe backend takes care of authenticating the user and writing the data points into an Amazon DynamoDB table.\nEvery morning, you scan the table to extract and aggregate last night's data on a per user basis, and store the results in Amazon S3. Users are notified via Amazon SNS mobile push notifications that new data is available, which is parsed and visualized by the mobile app.\nCurrently you have around 100k users who are mostly based out of North America.\nYou have been tasked to optimize the architecture of the backend system to lower cost.\nWhat would you recommend? (Choose 2)",
			"type": "checkbox",
			"showNumAnswers": true,
			"options": [
				"Have the mobile app access Amazon DynamoDB directly Instead of JSON files stored on Amazon S3",
				"Write data directly into an Amazon Redshift cluster replacing both Amazon DynamoDB and Amazon S3",
				"Introduce an Amazon SQS queue to buffer writes to the Amazon DynamoDB table and reduce provisioned write throughput",
				"Introduce Amazon Elasticache to cache reads from the Amazon DynamoDB table and reduce provisioned read throughput",
				"Create a new Amazon DynamoDB table each day and drop the one for the previous day after its data is on Amazon S3"
			],
			"answers": [
				"Introduce an Amazon SQS queue to buffer writes to the Amazon DynamoDB table and reduce provisioned write throughput",
				"Introduce Amazon Elasticache to cache reads from the Amazon DynamoDB table and reduce provisioned read throughput"
			],
			"answerDescription": "https://d0.awsstatic.com/whitepapers/performance-at-scale-with-amazon-elasticache.pdf"
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "A large real-estate brokerage is exploring the option of adding a cost-effective location based alert to their existing mobile application. The application backend infrastructure currently runs on AWS. Users who opt in to this service will receive alerts on their mobile device regarding real-estate otters in proximity to their location. For the alerts to be relevant delivery time needs to be in the low minute count the existing mobile app has 5 million users across the US.\nWhich one of the following architectural suggestions would you make to the customer?",
			"type": "radio",
			"showNumAnswers": false,
			"options": [
				"The mobile application will submit its location to a web service endpoint utilizing Elastic Load Balancing and EC2 instances; DynamoDB will be used to store and retrieve relevant offers EC2 instances will communicate with mobile earners/device providers to push alerts back to mobile application",
				"Use AWS DirectConnect or VPN to establish connectivity with mobile carriers EC2 instances will receive the mobile applications location through carrier connection: RDS will be used to store and relevant offers. EC2 instances will communicate with mobile carriers to push alerts back to the mobile application",
				"The mobile application will send device location using SQS. EC2 instances will retrieve the relevant others from DynamoDB. AWS Mobile Push will be used to send offers to the mobile application",
				"The mobile application will send device location using AWS Mobile Push EC2 instances will retrieve the relevant offers from DynamoDB. EC2 instances will communicate with mobile carriers/device providers to push alerts back to the mobile application"
			],
			"answers": [
				"The mobile application will send device location using SQS. EC2 instances will retrieve the relevant others from DynamoDB. AWS Mobile Push will be used to send offers to the mobile application"
			],
			"answerDescription": ""
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "You currently operate a web application. In the AWS US-East region. The application runs on an autoscaled layer of EC2 instances and an RDS Multi-AZ database. Your IT security compliance officer has tasked you to develop a reliable and durable logging solution to track changes made to your EC2.IAM And RDS resources. The solution must ensure the integrity and confidentiality of your log data.\nWhich of these solutions would you recommend?",
			"type": "radio",
			"showNumAnswers": false,
			"options": [
				"Create a new CloudTrail trail with one new S3 bucket to store the logs and with the global services option selected. Use IAM roles S3 bucket policies and Multi Factor Authentication (MFA) Delete on the S3 bucket that stores your logs",
				"Create a new CloudTrail with one new S3 bucket to store the logs Configure SNS to send log file delivery notifications to your management system. Use IAM roles and S3 bucket policies on the S3 bucket mat stores your logs",
				"Create a new CloudTrail trail with an existing S3 bucket to store the logs and with the global services option selected. Use S3 ACLs and Multi Factor Authentication (MFA). Delete on the S3 bucket that stores your logs",
				"Create three new CloudTrail trails with three new S3 buckets to store the logs one for the AWS Management console, one for AWS SDKs and one for command line tools. Use IAM roles and S3 bucket policies on the S3 buckets that store your logs"
			],
			"answers": [
				"Create a new CloudTrail trail with one new S3 bucket to store the logs and with the global services option selected. Use IAM roles S3 bucket policies and Multi Factor Authentication (MFA) Delete on the S3 bucket that stores your logs"
			],
			"answerDescription": ""
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "Your department creates regular analytics reports from your company's log files All log data is collected in Amazon S3 and processed by daily Amazon Elastic MapReduce (EMR) jobs that generate daily PDF reports and aggregated tables in CSV format for an Amazon Redshift data warehouse. Your CFO requests that you optimize the cost structure for this system.\nWhich of the following alternatives will lower costs without compromising average performance of the system or data integrity for the raw data?",
			"type": "radio",
			"showNumAnswers": false,
			"options": [
				"Use reduced redundancy storage (RRS) for all data In S3. Use a combination of Spot Instances and Reserved Instances for Amazon EMR jobs. Use Reserved Instances for Amazon Redshift",
				"Use reduced redundancy storage (RRS) for PDF and .csv data in S3. Add Spot Instances to EMR jobs. Use Spot Instances for Amazon Redshift",
				"Use reduced redundancy storage (RRS) for PDF and .csv data In Amazon S3. Add Spot Instances to Amazon EMR jobs. Use Reserved Instances for Amazon Redshift",
				"Use reduced redundancy storage (RRS) for all data in Amazon S3. Add Spot Instances to Amazon EMR jobs. Use Reserved Instances for Amazon Redshift"
			],
			"answers": [
				"Use reduced redundancy storage (RRS) for PDF and .csv data In Amazon S3. Add Spot Instances to Amazon EMR jobs. Use Reserved Instances for Amazon Redshift"
			],
			"answerDescription": ""
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "You require the ability to analyze a large amount of data, which is stored on Amazon S3 using Amazon Elastic Map Reduce. You are using the cc2 8xlarge instance type, whose CPUs are mostly idle during processing. Which of the below would be the most cost efficient way to reduce the runtime of the job?",
			"type": "radio",
			"showNumAnswers": false,
			"options": [
				"Create more, smaller flies on Amazon S3",
				"Add additional cc2 8xlarge instances by introducing a task group",
				"Use smaller instances that have higher aggregate I/O performance",
				"Create fewer, larger files on Amazon S3"
			],
			"answers": [
				"Use smaller instances that have higher aggregate I/O performance"
			],
			"answerDescription": ""
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "An AWS customer is deploying an application mat is composed of an AutoScaling group of EC2 Instances. The customers security policy requires that every outbound connection from these instances to any other service within the customers Virtual Private Cloud must be authenticated using a unique x 509 certificate that contains the specific instance-id.\nIn addition, an x 509 certificates must Designed by the customer's Key management service in order to be trusted for authentication.\nWhich of the following configurations will support these requirements?",
			"type": "radio",
			"showNumAnswers": false,
			"options": [
				"Configure an IAM Role that grants access to an Amazon S3 object containing a signed certificate and configure the Auto Scaling group to launch instances with this role. Have the instances bootstrap get the certificate from Amazon S3 upon first boot",
				"Embed a certificate into the Amazon Machine Image that is used by the Auto Scaling group. Have the launched instances generate a certificate signature request with the instance's assigned instance-id to the key management service for signature",
				"Configure the Auto Scaling group to send an SNS notification of the launch of a new instance to the trusted key management service. Have the Key management service generate a signed certificate and send it directly to the newly launched instance",
				"Configure the launched instances to generate a new certificate upon first boot. Have the Key management service poll the Auto Scaling group for associated instances and send new instances a certificate signature (hat contains the specific instance-id)"
			],
			"answers": [
				"Configure an IAM Role that grants access to an Amazon S3 object containing a signed certificate and configure the Auto Scaling group to launch instances with this role. Have the instances bootstrap get the certificate from Amazon S3 upon first boot"
			],
			"answerDescription": ""
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "Your company runs a customer facing event registration site This site is built with a 3-tier architecture with web and application tier servers and a MySQL database The application requires 6 web tier servers and 6 application tier servers for normal operation, but can run on a minimum of 65% server capacity and a single MySQL database.\nWhen deploying this application in a region with three availability zones (AZs) which architecture provides high availability?",
			"type": "radio",
			"showNumAnswers": false,
			"options": [
				"A web tier deployed across 2 AZs with 3 EC2 (Elastic Compute Cloud) instances in each AZ inside an Auto Scaling Group behind an ELB (elastic load balancer), and an application tier deployed across 2 AZs with 3 EC2 instances in each AZ inside an Auto Scaling Group behind an ELB and one RDS (Relational Database Service) instance deployed with read replicas in the other AZ",
				"A web tier deployed across 3 AZs with 2 EC2 (Elastic Compute Cloud) instances in each AZ inside an Auto Scaling Group behind an ELB (elastic load balancer) and an application tier deployed across 3 AZs with 2 EC2 instances in each AZ inside an Auto Scaling Group behind an ELB and one RDS (Relational Database Service) Instance deployed with read replicas in the two other AZs",
				"A web tier deployed across 2 AZs with 3 EC2 (Elastic Compute Cloud) instances in each AZ inside an Auto Scaling Group behind an ELB (elastic load balancer) and an application tier deployed across 2 AZs with 3 EC2 instances m each AZ inside an Auto Scaling Group behind an ELS and a Multi-AZ RDS (Relational Database Service) deployment",
				"A web tier deployed across 3 AZs with 2 EC2 (Elastic Compute Cloud) instances in each AZ Inside an Auto Scaling Group behind an ELB (elastic load balancer). And an application tier deployed across 3 AZs with 2 EC2 instances in each AZ inside an Auto Scaling Group behind an ELB and a Multi-AZ RDS (Relational Database services) deployment"
			],
			"answers": [
				"A web tier deployed across 3 AZs with 2 EC2 (Elastic Compute Cloud) instances in each AZ Inside an Auto Scaling Group behind an ELB (elastic load balancer). And an application tier deployed across 3 AZs with 2 EC2 instances in each AZ inside an Auto Scaling Group behind an ELB and a Multi-AZ RDS (Relational Database services) deployment"
			],
			"answerDescription": "Amazon RDS Multi-AZ Deployments\nAmazon RDS Multi-AZ deployments provide enhanced availability and durability for Database (DB) Instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Each AZ runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable. In case of an infrastructure failure (for example, instance hardware failure, storage failure, or network disruption), Amazon RDS performs an automatic failover to the standby, so that you can resume database operations as soon as the failover is complete. Since the endpoint for your DB Instance remains the same after a failover, your application can resume database operation without the need for manual administrative intervention. Enhanced Durability\nMulti-AZ deployments for the MySQL, Oracle, and PostgreSQL engines utilize synchronous physical replication to keep data on the standby up-to-date with the primary. Multi-AZ deployments for the SQL Server engine use synchronous logical replication to achieve the same result, employing SQL Servernative Mirroring technology. Both approaches safeguard your data in the event of a DB Instance failure or loss of an Availability Zone.\nIf a storage volume on your primary fails in a Multi-AZ deployment, Amazon RDS automatically initiates a failover to the up-to-date standby. Compare this to a Single-AZ deployment: in case of a Single-AZ database failure, a user-initiated point-in-time-restore operation will be required. This operation can take several hours to complete, and any data updates that occurred after the latest restorable time (typically within the last five minutes) will not be available.\nAmazon Aurora employs a highly durable, SSD-backed virtualized storage layer purpose-built for database workloads. Amazon Aurora automatically replicates your volume six ways, across three Availability Zones. Amazon Aurora storage is fault-tolerant, transparently handling the loss of up to two copies of data without affecting database write availability and up to three copies without affecting read availability. Amazon Aurora storage is also self-healing. Data blocks and disks are continuously scanned for errors and replaced automatically.\nIncreased Availability\nYou also benefit from enhanced database availability when running Multi-AZ deployments. If an Availability Zone failure or DB Instance failure occurs, your availability impact is limited to the time automatic failover takes to complete: typically under one minute for Amazon Aurora and one to two minutes for other database engines (see the RDS FAQ for details).\nThe availability benefits of Multi-AZ deployments also extend to planned maintenance and backups. In the case of system upgrades like OS patching or DB Instance scaling, these operations are applied first on the standby, prior to the automatic failover. As a result, your availability impact is, again, only the time required for automatic failover to complete.\nUnlike Single-AZ deployments, I/O activity is not suspended on your primary during backup for Multi-AZ deployments for the MySQL, Oracle, and PostgreSQL engines, because the backup is taken from the standby. However, note that you may still experience elevated latencies for a few minutes during backups for Multi-AZ deployments.\nOn instance failure in Amazon Aurora deployments, Amazon RDS uses RDS Multi-AZ technology to automate failover to one of up to 15 Amazon Aurora Replicas you have created in any of three Availability Zones. If no Amazon Aurora Replicas have been provisioned, in the case of a failure, Amazon RDS will attempt to create a new Amazon Aurora DB instance for you automatically."
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "Your customer wishes to deploy an enterprise application to AWS, which will consist of several web servers, several application servers and a small (50GB) Oracle database. Information is stored, both in the database and the file systems of the various servers. The backup system must support database recovery whole server and whole disk restores, and individual file restores with a recovery time of no more than two hours. They have chosen to use RDS Oracle as the database.\nWhich backup architecture will meet these requirements?",
			"type": "radio",
			"showNumAnswers": false,
			"options": [
				"Backup RDS using automated daily DB backups. Backup the EC2 instances using AMIs and supplement with file-level backup to S3 using traditional enterprise backup software to provide file level restore",
				"Backup RDS using a Multi-AZ Deployment. Backup the EC2 instances using Amis, and supplement by copying file system data to S3 to provide file level restore",
				"Backup RDS using automated daily DB backups. Backup the EC2 instances using EBS snapshots and supplement with file-level backups to Amazon Glacier using traditional enterprise backup software to provide file level restore",
				"Backup RDS database to S3 using Oracle RMAN. Backup the EC2 instances using Amis, and supplement with EBS snapshots for individual volume restore"
			],
			"answers": [
				"Backup RDS using automated daily DB backups. Backup the EC2 instances using AMIs and supplement with file-level backup to S3 using traditional enterprise backup software to provide file level restore"
			],
			"answerDescription": "Point-In-Time Recovery\nIn addition to the daily automated backup, Amazon RDS archives database change logs. This enables you to recover your database to any point in time during the backup retention period, up to the last five minutes of database usage.\nAmazon RDS stores multiple copies of your data, but for Single-AZ DB instances these copies are stored in a single availability zone. If for any reason a Single-AZ DB instance becomes unusable, you can use point-in-time recovery to launch a new DB instance with the latest restorable data. For more information on working with point-in-time recovery, go to Restoring a DB Instance to a Specified Time.\nNote\nMulti-AZ deployments store copies of your data in different Availability Zones for greater levels of data durability. For more information on Multi-AZ deployments, see High Availability (Multi-AZ)."
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "Your company has HQ in Tokyo and branch offices all over the world and is using a logistics software with a multi-regional deployment on AWS in Japan, Europe and USA. The logistic software has a 3-tier architecture and currently uses MySQL 5.6 for data persistence. Each region has deployed its own database.\nIn the HQ region you run an hourly batch process reading data from every region to compute crossregional reports that are sent by email to all offices this batch process must be completed as fast as possible to quickly optimize logistics.\nHow do you build the database architecture in order to meet the requirements?",
			"type": "radio",
			"showNumAnswers": false,
			"options": [
				"For each regional deployment, use RDS MySQL with a master in the region and a read replica in the HQ region",
				"For each regional deployment, use MySQL on EC2 with a master in the region and send hourly EBS snapshots to the HQ region",
				"For each regional deployment, use RDS MySQL with a master in the region and send hourly RDS snapshots to the HQ region",
				"For each regional deployment, use MySQL on EC2 with a master in the region and use S3 to copy data files hourly to the HQ region",
				"Use Direct Connect to connect all regional MySQL deployments to the HQ region and reduce network latency for the batch process"
			],
			"answers": [
				"For each regional deployment, use RDS MySQL with a master in the region and a read replica in the HQ region"
			],
			"answerDescription": ""
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "A web design company currently runs several FTP servers that their 250 customers use to upload and download large graphic files They wish to move this system to AWS to make it more scalable, but they wish to maintain customer privacy and Keep costs to a minimum.\n What AWS architecture would you recommend?",
			"type": "radio",
			"showNumAnswers": false,
			"options": [
				"ASK their customers to use an S3 client instead of an FTP client. Create a single S3 bucket Create an IAM user for each customer Put the IAM Users in a Group that has an IAM policy that permits access to sub-directories within the bucket via use of the 'username' Policy variable",
				"Create a single S3 bucket with Reduced Redundancy Storage turned on and ask their customers to use an S3 client instead of an FTP client Create a bucket for each customer with a Bucket Policy that permits access only to that one customer",
				"Create an auto-scaling group of FTP servers with a scaling policy to automatically scale-in when minimum network traffic on the auto-scaling group is below a given threshold. Load a central list of ftp users from S3 as part of the user Data startup script on each Instance",
				"Create a single S3 bucket with Requester Pays turned on and ask their customers to use an S3 client instead of an FTP client Create a bucket tor each customer with a Bucket Policy that permits access only to that one customer"
			],
			"answers": [
				"ASK their customers to use an S3 client instead of an FTP client. Create a single S3 bucket Create an IAM user for each customer Put the IAM Users in a Group that has an IAM policy that permits access to sub-directories within the bucket via use of the 'username' Policy variable"
			],
			"answerDescription": ""
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "You would like to create a mirror image of your production environment in another region for disaster recovery purposes. \nWhich of the following AWS resources do not need to be recreated in the second region? (Choose 2 answers)",
			"type": "checkbox",
			"showNumAnswers": true,
			"options": [
				"Route 53 Record Sets",
				"IAM Roles",
				"Elastic IP Addresses (EIP)",
				"EC2 Key Pairs",
				"Launch configurations",
				"Security Groups"
			],
			"answers": [
				"Route 53 Record Sets",
				"IAM Roles"
			],
			"answerDescription": "As per the document defined, new IPs should be reserved not the same ones Elastic IP Addresses are static IP addresses designed for dynamic cloud computing. Unlike traditional static IP addresses, however, Elastic IP addresses enable you to mask instance or Availability Zone failures by programmatically remapping your public IP addresses to instances in your account in a particular region. For DR, you can also pre-allocate some IP addresses for the most critical systems so that their IP addresses are already known before disaster strikes. This can simplify the execution of the DR plan.\nReference: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/resources.html"
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "Your company currently has a 2-tier web application running in an on-premises data center. You have experienced several infrastructure failures in the past two months resulting in significant financial losses. Your CIO is strongly agreeing to move the application to AWS. While working on achieving buy-in from the other company executives, he asks you to develop a disaster recovery plan to help improve Business continuity in the short term. He specifies a target Recovery Time Objective (RTO) of 4 hours and a Recovery Point Objective (RPO) of 1 hour or less. He also asks you to implement the solution within 2 weeks.\nYour database is 200GB in size and you have a 20Mbps Internet connection. How would you do this while minimizing costs?",
			"type": "radio",
			"showNumAnswers": false,
			"options": [
				"Create an EBS backed private AMI which includes a fresh install of your application. Develop a CloudFormation template which includes your AMI and the required EC2, AutoScaling, and ELB resources to support deploying the application across Multiple- Availability-Zones. Asynchronously replicate transactions from your on-premises database to a database instance in AWS across a secure VPN connection",
				"Deploy your application on EC2 instances within an Auto Scaling group across multiple availability zones. Asynchronously replicate transactions from your on-premises database to a database instance in AWS across a secure VPN connection",
				"Create an EBS backed private AMI which includes a fresh install of your application. Setup a script in your data center to backup the local database every 1 hour and to encrypt and copy the resulting file to an S3 bucket using multi-part upload",
				"Install your application on a compute-optimized EC2 instance capable of supporting the application's average load. Synchronously replicate transactions from your on-premises database to a database instance in AWS across a secure Direct Connect connection"
			],
			"answers": [
				"Create an EBS backed private AMI which includes a fresh install of your application. Develop a CloudFormation template which includes your AMI and the required EC2, AutoScaling, and ELB resources to support deploying the application across Multiple- Availability-Zones. Asynchronously replicate transactions from your on-premises database to a database instance in AWS across a secure VPN connection"
			],
			"answerDescription": "Overview of Creating Amazon EBS-Backed AMIs\n First, launch an instance from an AMI that's similar to the AMI that you'd like to create. You can connect to your instance and customize it. When the instance is configured correctly, ensure data integrity by stopping the instance before you create an AMI, then create the image. When you create an Amazon EBS-backed AMI, we automatically register it for you.\n Amazon EC2 powers down the instance before creating the AMI to ensure that everything on the instance is stopped and in a consistent state during the creation process. If you're confident that your instance is in a consistent state appropriate for AMI creation, you can tell Amazon EC2 not to power down and reboot the instance. Some file systems, such as XFS, can freeze and unfreeze activity, making it safe to create the image without rebooting the instance.\n During the AMI-creation process, Amazon EC2 creates snapshots of your instance's root volume and any other EBS volumes attached to your instance. If any volumes attached to the instance are encrypted, the new AMI only launches successfully on instances that support Amazon EBS encryption. For more information, see Amazon EBS Encryption.\n Depending on the size of the volumes, it can take several minutes for the AMI-creation process to complete (sometimes up to 24 hours). You may find it more efficient to create snapshots of your volumes prior to creating your AMI. This way, only small, incremental snapshots need to be created when the AMI iscreated, and the process completes more quickly (the total time for snapshot creation remains the same). For more information, see Creating an Amazon EBS Snapshot.\n After the process completes, you have a new AMI and snapshot created from the root volume of the instance. When you launch an instance using the new AMI, we create a new EBS volume for its root volume using the snapshot. Both the AMI and the snapshot incur charges to your account until you delete them. For more information, see Deregistering Your AMI.\n If you add instance-store volumes or EBS volumes to your instance in addition to the root device volume, the block device mapping for the new AMI contains information for these volumes, and the block device mappings for instances that you launch from the new AMI automatically contain information for these volumes. The instance-store volumes specified in the block device mapping for the new instance are new and don't contain any data from the instance store volumes of the instance you used to create the AMI. The data on EBS volumes persists. For more information, see Block Device Mapping"
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "An enterprise wants to use a third-party SaaS application. The SaaS application needs to have access to issue several API commands to discover Amazon EC2 resources running within the enterprise's account The enterprise has internal security policies that require any outside access to their environment must conform to the principles of least privilege and there must be controls in place to ensure that the credentials used by the SaaS vendor cannot be used by any other third party.\nWhich of the following would meet all of these conditions?",
			"type": "radio",
			"showNumAnswers": false,
			"options": [
				"From the AWS Management Console, navigate to the Security Credentials page and retrieve the access and secret key for your account",
				"Create an IAM user within the enterprise account assign a user policy to the IAM user that allows only the actions required by the SaaS application create a new access and secret key for the user and provide these credentials to the SaaS provider",
				"Create an IAM role for cross-account access allows the SaaS provider's account to assume the role and assign it a policy that allows only the actions required by the SaaS application",
				"Create an IAM role for EC2 instances, assign it a policy that allows only the actions required tor the SaaS application to work, provide the role ARN to the SaaS provider to use when launching their application instances"
			],
			"answers": [
				"Create an IAM role for cross-account access allows the SaaS provider's account to assume the role and assign it a policy that allows only the actions required by the SaaS application"
			],
			"answerDescription": ""
		},
		{
			"id": 1,
			"category": "",
			"level": "",
			"question": "A customer has a 10 GB AWS Direct Connect connection to an AWS region where they have a web application hosted on Amazon Elastic Computer Cloud (EC2). The application has dependencies on an onpremises mainframe database that uses a BASE (Basic Available, Soft state, Eventual consistency) rather than an ACID (Atomicity, Consistency, Isolation, Durability) consistency model. The application is exhibiting undesirable behavior because the database is not able to handle the volume of writes.\nHow can you reduce the load on your on-premises database resources in the most cost-effective way?",
			"type": "radio",
			"showNumAnswers": false,
			"options": [
				"Use an Amazon Elastic Map Reduce (EMR) S3DistCp as a synchronization mechanism between the on-premises database and a Hadoop cluster on AWS",
				"Modify the application to write to an Amazon SQS queue and develop a worker process to flush the queue to the on-premises database",
				"Modify the application to use DynamoDB to feed an EMR cluster which uses a map function to write to the on-premises database",
				"Provision an RDS read-replica database on AWS to handle the writes and synchronize the two databases using Data Pipeline"
			],
			"answers": [
				"Use an Amazon Elastic Map Reduce (EMR) S3DistCp as a synchronization mechanism between the on-premises database and a Hadoop cluster on AWS"
			],
			"answerDescription": "https://aws.amazon.com/blogs/aws/category/amazon-elastic-map-reduce/"
		}
	]
}